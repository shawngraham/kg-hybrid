{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Knowledge Graph Extraction for Antiquities Trafficking\n",
    "\n",
    "This notebook implements a hybrid pipeline combining:\n",
    "- **spaCy** for fast entity extraction\n",
    "- **Gemini LLM** for coreference resolution and entity refinement\n",
    "- **Knowledge Graph** construction and visualization\n",
    "\n",
    "## Architecture\n",
    "1. Stage 1: Fast NLP extraction (spaCy) ‚Üí 259 entities in ~1s\n",
    "2. Stage 2: LLM refinement (Gemini) ‚Üí Coreference + Canonicalization\n",
    "3. Stage 3: Knowledge Graph construction\n",
    "4. Stage 4: Visualization and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q spacy google-generativeai networkx matplotlib pyvis\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set your Gemini API key\n",
    "# Get key from: https://makersuite.google.com/app/apikey\n",
    "GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your key\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Stage 1: Fast Entity Extraction (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_fast(text):\n",
    "    \"\"\"Stage 1: Fast entity extraction with spaCy\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            \"text\": ent.text,\n",
    "            \"label\": ent.label_,\n",
    "            \"start\": ent.start_char,\n",
    "            \"end\": ent.end_char,\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"entities\": entities,\n",
    "        \"text\": text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Stage 2: LLM Refinement Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Simple (Fast, Coreference Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_refine_entities_simple(raw_entities, text):\n",
    "    \"\"\"Simplified: Just coreference resolution\"\"\"\n",
    "    \n",
    "    entities = raw_entities[\"entities\"]\n",
    "    \n",
    "    # Focus on PERSON and ORG only\n",
    "    key_entities = [e for e in entities if e['label'] in ['PERSON', 'ORG', 'GPE']]\n",
    "    \n",
    "    print(f\"  Focusing on {len(key_entities)} PERSON/ORG entities...\")\n",
    "    \n",
    "    # Group by type\n",
    "    entity_list = {}\n",
    "    for e in key_entities:\n",
    "        label = e['label']\n",
    "        if label not in entity_list:\n",
    "            entity_list[label] = []\n",
    "        entity_list[label].append(e['text'])\n",
    "    \n",
    "    # Deduplicate\n",
    "    for label in entity_list:\n",
    "        entity_list[label] = list(set(entity_list[label]))\n",
    "    \n",
    "    entity_summary = \"\\n\\n\".join([\n",
    "        f\"{label}:\\n\" + \"\\n\".join([f\"  - {text}\" for text in texts[:50]])\n",
    "        for label, texts in entity_list.items()\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Identify which entity mentions refer to the same real-world entity in this antiquities trafficking document.\n",
    "\n",
    "DOCUMENT START:\n",
    "{text[:2000]}...\n",
    "\n",
    "ENTITIES FOUND:\n",
    "{entity_summary}\n",
    "\n",
    "Example output format:\n",
    "{{\n",
    "  \"giacomo_medici\": {{\n",
    "    \"full_name\": \"Giacomo Medici\",\n",
    "    \"mentions\": [\"Giacomo Medici\", \"Medici\"],\n",
    "    \"type\": \"PERSON\",\n",
    "    \"role\": \"dealer\"\n",
    "  }},\n",
    "  \"j_paul_getty_museum\": {{\n",
    "    \"full_name\": \"J. Paul Getty Museum\",\n",
    "    \"mentions\": [\"J. Paul Getty Museum\", \"Getty Museum\"],\n",
    "    \"type\": \"ORGANIZATION\",\n",
    "    \"role\": \"museum\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON object.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=4096,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"  Response length: {len(response.text)} chars\")\n",
    "        \n",
    "        # Parse JSON\n",
    "        clusters = json.loads(response.text)\n",
    "        \n",
    "        if not isinstance(clusters, dict):\n",
    "            print(f\"  ERROR: Expected dict, got {type(clusters)}\")\n",
    "            return {\"refined_entities\": [], \"entity_clusters\": {}, \"events\": []}\n",
    "        \n",
    "        # Convert to expected format\n",
    "        simplified_clusters = {}\n",
    "        refined_entities = []\n",
    "        \n",
    "        for canonical_id, data in clusters.items():\n",
    "            if not isinstance(data, dict):\n",
    "                continue\n",
    "                \n",
    "            mentions = data.get('mentions', [])\n",
    "            if not mentions:\n",
    "                continue\n",
    "                \n",
    "            simplified_clusters[canonical_id] = mentions\n",
    "            \n",
    "            for mention in mentions:\n",
    "                original = next((e for e in entities if e['text'] == mention), None)\n",
    "                if original:\n",
    "                    refined_entities.append({\n",
    "                        \"original_text\": mention,\n",
    "                        \"canonical_id\": canonical_id,\n",
    "                        \"entity_class\": data.get('type', 'UNKNOWN'),\n",
    "                        \"attributes\": {\n",
    "                            \"full_name\": data.get('full_name', ''),\n",
    "                            \"role\": data.get('role', '')\n",
    "                        },\n",
    "                        \"start\": original['start'],\n",
    "                        \"end\": original['end']\n",
    "                    })\n",
    "        \n",
    "        print(f\"  Successfully processed {len(simplified_clusters)} clusters\")\n",
    "        \n",
    "        return {\n",
    "            \"refined_entities\": refined_entities,\n",
    "            \"entity_clusters\": simplified_clusters,\n",
    "            \"events\": []\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return {\"refined_entities\": [], \"entity_clusters\": {}, \"events\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Batched (Comprehensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_refine_entities_batched(raw_entities, text, batch_size=30):\n",
    "    \"\"\"Process entities in batches\"\"\"\n",
    "    \n",
    "    all_refined = []\n",
    "    all_clusters = {}\n",
    "    all_events = []\n",
    "    \n",
    "    entities = raw_entities[\"entities\"]\n",
    "    num_batches = (len(entities) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"  Processing {len(entities)} entities in {num_batches} batches...\")\n",
    "    \n",
    "    for i in range(0, len(entities), batch_size):\n",
    "        batch = entities[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        print(f\"  Batch {batch_num}/{num_batches}...\", end=\" \")\n",
    "        \n",
    "        entity_summary = \"\\n\".join([f\"- {e['label']}: '{e['text']}'\" for e in batch])\n",
    "        \n",
    "        # Get context\n",
    "        min_start = min(e['start'] for e in batch)\n",
    "        max_end = max(e['end'] for e in batch)\n",
    "        context_start = max(0, min_start - 200)\n",
    "        context_end = min(len(text), max_end + 200)\n",
    "        context = text[context_start:context_end]\n",
    "        \n",
    "        prompt = f\"\"\"Analyze these entities from an antiquities trafficking document.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ENTITIES:\n",
    "{entity_summary}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"refined_entities\": [\n",
    "    {{\"original_text\": \"...\", \"canonical_id\": \"...\", \"entity_class\": \"PERSON|ORG|ARTIFACT\", \"attributes\": {{}}, \"start\": 0, \"end\": 0}}\n",
    "  ],\n",
    "  \"entity_clusters\": {{\"canonical_id\": [\"mention1\", \"mention2\"]}},\n",
    "  \"events\": []\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=0.1,\n",
    "                    max_output_tokens=8192,\n",
    "                    response_mime_type=\"application/json\",\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.text)\n",
    "            all_refined.extend(result.get('refined_entities', []))\n",
    "            all_clusters.update(result.get('entity_clusters', {}))\n",
    "            all_events.extend(result.get('events', []))\n",
    "            print(\"‚úì\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        \"refined_entities\": all_refined,\n",
    "        \"entity_clusters\": all_clusters,\n",
    "        \"events\": all_events\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: spaCy Only (No LLM, Fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kg_from_spacy_only(raw_entities, text):\n",
    "    \"\"\"Create KG directly from spaCy without LLM refinement\"\"\"\n",
    "    \n",
    "    entities = raw_entities[\"entities\"]\n",
    "    clusters = {}\n",
    "    \n",
    "    for e in entities:\n",
    "        if e['label'] == 'PERSON':\n",
    "            # Use last word as canonical ID\n",
    "            words = e['text'].split()\n",
    "            canonical = words[-1].lower().replace('.', '') if words else e['text'].lower()\n",
    "            \n",
    "            if canonical not in clusters:\n",
    "                clusters[canonical] = []\n",
    "            clusters[canonical].append(e['text'])\n",
    "        elif e['label'] in ['ORG', 'GPE', 'FAC']:\n",
    "            canonical = e['text'].lower().replace(' ', '_').replace('.', '')\n",
    "            if canonical not in clusters:\n",
    "                clusters[canonical] = []\n",
    "            clusters[canonical].append(e['text'])\n",
    "    \n",
    "    # Build refined entities\n",
    "    refined_entities = []\n",
    "    for canonical_id, mentions in clusters.items():\n",
    "        for mention in mentions:\n",
    "            original = next((e for e in entities if e['text'] == mention), None)\n",
    "            if original:\n",
    "                refined_entities.append({\n",
    "                    \"original_text\": mention,\n",
    "                    \"canonical_id\": canonical_id,\n",
    "                    \"entity_class\": original['label'],\n",
    "                    \"attributes\": {\"full_name\": mention},\n",
    "                    \"start\": original['start'],\n",
    "                    \"end\": original['end']\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"refined_entities\": refined_entities,\n",
    "        \"entity_clusters\": clusters,\n",
    "        \"events\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Stage 3: Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(raw_entities, refined_entities, entity_clusters, events):\n",
    "    \"\"\"Construct knowledge graph from hybrid extraction\"\"\"\n",
    "    \n",
    "    def get_position(mention_text, raw_entities):\n",
    "        for entity in raw_entities:\n",
    "            if entity['text'] == mention_text:\n",
    "                return (entity['start'], entity['end'])\n",
    "        return None\n",
    "    \n",
    "    nodes = {}\n",
    "    edges = []\n",
    "    \n",
    "    # Create nodes\n",
    "    for canonical_id, mentions in entity_clusters.items():\n",
    "        entity_data = next(\n",
    "            (e for e in refined_entities if e.get(\"canonical_id\") == canonical_id),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        if entity_data is None:\n",
    "            first_mention = mentions[0] if mentions else canonical_id\n",
    "            entity_data = {\n",
    "                \"canonical_id\": canonical_id,\n",
    "                \"entity_class\": \"UNKNOWN\",\n",
    "                \"attributes\": {\"full_name\": first_mention}\n",
    "            }\n",
    "        \n",
    "        nodes[canonical_id] = {\n",
    "            \"id\": canonical_id,\n",
    "            \"type\": entity_data.get(\"entity_class\", \"UNKNOWN\"),\n",
    "            \"label\": entity_data.get(\"attributes\", {}).get(\"full_name\", canonical_id),\n",
    "            \"mentions\": mentions,\n",
    "            \"attributes\": entity_data.get(\"attributes\", {}),\n",
    "            \"source_positions\": [\n",
    "                {\"mention\": m, \"position\": get_position(m, raw_entities)}\n",
    "                for m in mentions\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Create edges from events\n",
    "    for event in events:\n",
    "        event_entities = event.get(\"entities\", [])\n",
    "        event_id = event.get(\"event_id\", \"unknown\")\n",
    "        event_type = event.get(\"event_type\", \"related_to\")\n",
    "        \n",
    "        for i, e1 in enumerate(event_entities):\n",
    "            for e2 in event_entities[i+1:]:\n",
    "                edges.append({\n",
    "                    \"source\": e1,\n",
    "                    \"target\": e2,\n",
    "                    \"relation\": event_type,\n",
    "                    \"event_id\": event_id\n",
    "                })\n",
    "    \n",
    "    # If no events, create co-occurrence edges\n",
    "    if not edges:\n",
    "        node_ids = list(nodes.keys())\n",
    "        for i, n1 in enumerate(node_ids):\n",
    "            for n2 in node_ids[i+1:min(i+6, len(node_ids))]:\n",
    "                edges.append({\n",
    "                    \"source\": n1,\n",
    "                    \"target\": n2,\n",
    "                    \"relation\": \"co_occurs_with\",\n",
    "                    \"event_id\": \"implicit\"\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"summary\": {\n",
    "            \"num_nodes\": len(nodes),\n",
    "            \"num_edges\": len(edges),\n",
    "            \"node_types\": list(set(n[\"type\"] for n in nodes.values()))\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_kg(kg, max_nodes=20):\n",
    "    \"\"\"Pretty print the knowledge graph\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KNOWLEDGE GRAPH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"   Nodes: {kg['summary']['num_nodes']}\")\n",
    "    print(f\"   Edges: {kg['summary']['num_edges']}\")\n",
    "    print(f\"   Types: {', '.join(kg['summary']['node_types'])}\")\n",
    "    \n",
    "    print(f\"\\nüë§ NODES (showing first {max_nodes}):\")\n",
    "    for i, (node_id, node_data) in enumerate(list(kg['nodes'].items())[:max_nodes]):\n",
    "        print(f\"\\n   {i+1}. {node_data['label']} ({node_data['type']})\")\n",
    "        print(f\"      ID: {node_id}\")\n",
    "        print(f\"      Mentions: {', '.join(node_data['mentions'][:5])}\")\n",
    "        if node_data['attributes']:\n",
    "            print(f\"      Attributes: {node_data['attributes']}\")\n",
    "    \n",
    "    print(f\"\\nüîó EDGES (showing first 20):\")\n",
    "    for i, edge in enumerate(kg['edges'][:20]):\n",
    "        source_label = kg['nodes'][edge['source']]['label']\n",
    "        target_label = kg['nodes'][edge['target']]['label']\n",
    "        print(f\"   {i+1}. {source_label} --[{edge['relation']}]--> {target_label}\")\n",
    "    \n",
    "    if len(kg['edges']) > 20:\n",
    "        print(f\"   ... and {len(kg['edges']) - 20} more edges\")\n",
    "\n",
    "\n",
    "def visualize_kg_simple(kg, output_file=\"kg_viz.png\"):\n",
    "    \"\"\"Create simple matplotlib visualization\"\"\"\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for node_id, node_data in kg['nodes'].items():\n",
    "        G.add_node(node_id, label=node_data['label'], type=node_data['type'])\n",
    "    \n",
    "    # Add edges\n",
    "    for edge in kg['edges']:\n",
    "        if edge['source'] in G.nodes and edge['target'] in G.nodes:\n",
    "            G.add_edge(edge['source'], edge['target'], relation=edge['relation'])\n",
    "    \n",
    "    # Create layout\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    \n",
    "    # Color nodes by type\n",
    "    color_map = {\n",
    "        \"PERSON\": \"#FF6B6B\",\n",
    "        \"ORG\": \"#4ECDC4\", \n",
    "        \"ORGANIZATION\": \"#4ECDC4\",\n",
    "        \"ARTIFACT\": \"#FFE66D\",\n",
    "        \"GPE\": \"#95E1D3\",\n",
    "        \"LOCATION\": \"#95E1D3\",\n",
    "        \"UNKNOWN\": \"#CCCCCC\"\n",
    "    }\n",
    "    \n",
    "    node_colors = [color_map.get(kg['nodes'][node]['type'], \"#CCCCCC\") for node in G.nodes()]\n",
    "    \n",
    "    # Draw\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1000, alpha=0.9)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, width=2)\n",
    "    \n",
    "    # Labels\n",
    "    labels = {node: kg['nodes'][node]['label'][:20] for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title(f\"Knowledge Graph: {kg['summary']['num_nodes']} nodes, {kg['summary']['num_edges']} edges\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Visualization saved to {output_file}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_kg_json(kg, filename=\"knowledge_graph.json\"):\n",
    "    \"\"\"Save knowledge graph as JSON\"\"\"\n",
    "    \n",
    "    kg_serializable = {\n",
    "        \"nodes\": [{\"id\": node_id, **node_data} for node_id, node_data in kg['nodes'].items()],\n",
    "        \"edges\": kg['edges'],\n",
    "        \"summary\": kg['summary']\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(kg_serializable, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Knowledge graph saved to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_kg_extraction(document_text, mode=\"spacy_only\"):\n",
    "    \"\"\"Full hybrid extraction pipeline\n",
    "    \n",
    "    Args:\n",
    "        mode: \"spacy_only\" (fastest), \"simple\" (LLM coreference), \"batched\" (full LLM)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Stage 1: spaCy extraction...\")\n",
    "    raw = extract_entities_fast(document_text)\n",
    "    print(f\"  Found {len(raw['entities'])} raw entities\")\n",
    "    \n",
    "    print(f\"\\nStage 2: LLM refinement (mode={mode})...\")\n",
    "    \n",
    "    if mode == \"spacy_only\":\n",
    "        refined = create_kg_from_spacy_only(raw, document_text)\n",
    "    elif mode == \"simple\":\n",
    "        refined = llm_refine_entities_simple(raw, document_text)\n",
    "    elif mode == \"batched\":\n",
    "        refined = llm_refine_entities_batched(raw, document_text, batch_size=30)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "    \n",
    "    print(f\"  Refined to {len(refined['refined_entities'])} entities\")\n",
    "    print(f\"  Identified {len(refined['entity_clusters'])} entity clusters\")\n",
    "    print(f\"  Found {len(refined['events'])} events\")\n",
    "    \n",
    "    return {\n",
    "        \"raw\": raw,\n",
    "        \"refined\": refined\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document text\n",
    "document_text = \"\"\"Giacomo Medici is an Italian antiquities dealer who was convicted in 2005 of receiving stolen goods, illegal export of goods, and conspiracy to traffic.\n",
    "\n",
    "Medici started dealing in antiquities in Rome during the 1960s. In July 1967, he was convicted in Italy of receiving looted artefacts, though in the same year he met and became an important supplier of antiquities to US dealer Robert Hecht. In 1968, Medici opened the gallery Antiquaria Romana in Rome and began to explore business opportunities in Switzerland.\n",
    "\n",
    "In 1978, he closed his Rome gallery, and entered into partnership with Geneva resident Christian Boursaud, who started consigning material supplied by Medici for sale at Sotheby's London. Together, they opened Hydra Gallery in Geneva in 1983.\n",
    "\n",
    "In October 1985, the Hydra Gallery sold fragments of the Onesimos kylix to the J. Paul Getty Museum for $100,000, providing a false provenance by way of the fictitious Zbinden collection. The Getty returned the kylix to Italy in 1999.\n",
    "\n",
    "On 13 September 1995, in concert with Swiss police, they raided Medici's storage space in the Geneva Freeport. In January 1997, Medici was arrested in Rome.\n",
    "\n",
    "Medici was charged with receiving stolen goods, illegal export of goods, and conspiracy to traffic. On 12 May 2005, he was found guilty of all charges. He was sentenced to ten years in prison and received a ‚Ç¨10 million fine.\"\"\"\n",
    "\n",
    "# Run the pipeline\n",
    "result = hybrid_kg_extraction(document_text, mode=\"spacy_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Build Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the knowledge graph\n",
    "kg = build_knowledge_graph(\n",
    "    result['raw']['entities'],\n",
    "    result['refined']['refined_entities'],\n",
    "    result['refined']['entity_clusters'],\n",
    "    result['refined']['events']\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "display_kg(kg, max_nodes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "visualize_kg_simple(kg, \"antiquities_kg.png\")\n",
    "\n",
    "# Save as JSON\n",
    "save_kg_json(kg, \"antiquities_kg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Query the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_kg(kg, query_type, **kwargs):\n",
    "    \"\"\"Simple query interface\"\"\"\n",
    "    \n",
    "    if query_type == \"find_person\":\n",
    "        name = kwargs.get('name', '').lower()\n",
    "        return [(nid, n) for nid, n in kg['nodes'].items()\n",
    "                if n['type'] == 'PERSON' and name in nid.lower()]\n",
    "    \n",
    "    elif query_type == \"connections\":\n",
    "        node_id = kwargs.get('node_id')\n",
    "        connections = [(e['target'], e['relation']) for e in kg['edges'] \n",
    "                      if e['source'] == node_id]\n",
    "        connections.extend([(e['source'], e['relation']) for e in kg['edges']\n",
    "                           if e['target'] == node_id])\n",
    "        return connections\n",
    "    \n",
    "    elif query_type == \"by_type\":\n",
    "        entity_type = kwargs.get('type')\n",
    "        return [(nid, n) for nid, n in kg['nodes'].items()\n",
    "                if n['type'] == entity_type]\n",
    "\n",
    "# Example queries\n",
    "print(\"\\nüîç QUERIES\")\n",
    "print(\"\\nAll people:\")\n",
    "people = query_kg(kg, \"by_type\", type=\"PERSON\")\n",
    "for pid, person in people[:10]:\n",
    "    print(f\"  - {person['label']}\")\n",
    "\n",
    "print(\"\\nAll organizations:\")\n",
    "orgs = query_kg(kg, \"by_type\", type=\"ORG\")\n",
    "for oid, org in orgs[:10]:\n",
    "    print(f\"  - {org['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Use Your Own Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your document\n",
    "# Option 1: From file\n",
    "# with open('your_document.txt', 'r', encoding='utf-8') as f:\n",
    "#     your_document = f.read()\n",
    "\n",
    "# Option 2: Paste directly\n",
    "your_document = \"\"\"Paste your antiquities trafficking document here...\"\"\"\n",
    "\n",
    "# Run extraction\n",
    "# Try \"spacy_only\" first (fastest), then \"simple\" or \"batched\" if you need better coreference\n",
    "result = hybrid_kg_extraction(your_document, mode=\"spacy_only\")\n",
    "\n",
    "# Build graph\n",
    "kg = build_knowledge_graph(\n",
    "    result['raw']['entities'],\n",
    "    result['refined']['refined_entities'],\n",
    "    result['refined']['entity_clusters'],\n",
    "    result['refined']['events']\n",
    ")\n",
    "\n",
    "# Display and save\n",
    "display_kg(kg)\n",
    "visualize_kg_simple(kg, \"my_kg.png\")\n",
    "save_kg_json(kg, \"my_kg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### Three Modes Available:\n",
    "\n",
    "1. **`spacy_only`** (Recommended Start)\n",
    "   - Fastest (1-2 seconds)\n",
    "   - No LLM calls\n",
    "   - Simple last-name based coreference\n",
    "   - Good for quick exploration\n",
    "\n",
    "2. **`simple`** (Better Coreference)\n",
    "   - Medium speed (5-10 seconds)\n",
    "   - 1 LLM call for coreference resolution\n",
    "   - Better entity linking\n",
    "   - Good for production\n",
    "\n",
    "3. **`batched`** (Full Feature)\n",
    "   - Slower (30-60 seconds)\n",
    "   - Multiple LLM calls\n",
    "   - Full domain classification + events\n",
    "   - Best quality\n",
    "\n",
    "### Next Steps:\n",
    "- Scale to multiple documents\n",
    "- Fine-tune spaCy NER model on antiquities domain\n",
    "- Export to Neo4j for graph database queries\n",
    "- Add relationship extraction rules\n",
    "- Create interactive web dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
