{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Knowledge Graph Extraction for Antiquities Trafficking\n",
    "\n",
    "This notebook implements domain-specific extraction using:\n",
    "- **Structured entity schema** (PERSON, ORGANIZATION, ARTIFACT, TRANSACTION, etc.)\n",
    "- **Coreference resolution** with canonical_id\n",
    "- **Event linking** using shared event_id\n",
    "- **Order-preserving extraction** from text\n",
    "- **Rich attributes** for domain context\n",
    "\n",
    "## Architecture\n",
    "1. Stage 1: LLM-based structured extraction with domain schema\n",
    "2. Stage 2: Knowledge Graph construction\n",
    "3. Stage 3: Visualization and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai networkx matplotlib pyvis anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Choose your LLM provider\n",
    "USE_ANTHROPIC = True  # Set to False to use Gemini instead\n",
    "\n",
    "if USE_ANTHROPIC:\n",
    "    from anthropic import Anthropic\n",
    "    ANTHROPIC_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your key\n",
    "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "    print(\"‚úÖ Using Claude (Anthropic)\")\n",
    "else:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your key\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "    print(\"‚úÖ Using Gemini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Extraction Schema and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_SCHEMA = \"\"\"\n",
    "## Entity Classes for Antiquities Trafficking\n",
    "\n",
    "Extract the following entity types IN ORDER OF APPEARANCE:\n",
    "\n",
    "1. **PERSON**: Individuals (dealers, collectors, officials, looters)\n",
    "   - Use canonical_id: firstname_lastname (lowercase, underscores)\n",
    "   - Add full_name when known\n",
    "   - Include role attribute (dealer, collector, official, looter, buyer, seller, suspect)\n",
    "   - Pronouns inherit canonical_id from context\n",
    "\n",
    "2. **ORGANIZATION**: Institutions (museums, galleries, auction houses, law enforcement)\n",
    "   - Use canonical_id (e.g., j_paul_getty_museum)\n",
    "   - Add entity_type (museum, gallery, auction_house, law_enforcement)\n",
    "   - Include entity_role when in a transaction (buyer, seller)\n",
    "\n",
    "3. **ARTIFACT**: Cultural objects being trafficked\n",
    "   - Use canonical_id (e.g., euphronios_sarpedon_krater)\n",
    "   - Add object_type (Greek pottery, sculpture, etc.)\n",
    "   - Include condition and legal_status when mentioned\n",
    "\n",
    "4. **CRIMINAL_ACTIVITY**: Illicit actions\n",
    "   - Use activity_type attribute: illicit excavation, illegal export, forgery, smuggling\n",
    "   - Link to artifact_id and perpetrator_id\n",
    "\n",
    "5. **TRANSACTION**: Action verbs (sold, bought, consigned, acquired)\n",
    "   - Extract the VERB ONLY as extraction_text\n",
    "   - Use transaction_type: sale, acquisition, consignment\n",
    "   - Include seller_id, buyer_id, artifact_id, amount, date\n",
    "\n",
    "6. **LEGAL_ACTION**: Prosecutions, raids, convictions\n",
    "   - Use action_type: conviction, raid, prosecution, arrest\n",
    "   - Include date, executing_authority, target_id\n",
    "\n",
    "7. **LOCATION**: Places relevant to trafficking network\n",
    "   - Use canonical_id (e.g., geneva_freeport)\n",
    "   - Add location_type and significance\n",
    "\n",
    "8. **PROVENANCE_CLAIM**: False or fabricated ownership histories\n",
    "   - Use claim_status: fabricated, disputed, verified\n",
    "   - Include claimed_source and artifact_id\n",
    "\n",
    "## Critical Rules:\n",
    "\n",
    "1. **Order of appearance**: Extract entities in the sequence they appear in text\n",
    "2. **Exact text spans**: Use the actual text from document, no overlap\n",
    "3. **Event linking**: Use shared event_id to connect related extractions\n",
    "4. **Coreference**: Different mentions of same entity get same canonical_id\n",
    "5. **Pronouns**: \"he\", \"she\", \"it\", \"they\" inherit canonical_id from context\n",
    "\n",
    "## Example:\n",
    "\n",
    "Text: \"In 1985, the Hydra Gallery sold fragments of the Onesimos kylix to the J. Paul Getty Museum for $100,000.\"\n",
    "\n",
    "Extractions:\n",
    "[\n",
    "  {\n",
    "    \"extraction_class\": \"ORGANIZATION\",\n",
    "    \"extraction_text\": \"Hydra Gallery\",\n",
    "    \"attributes\": {\n",
    "      \"canonical_id\": \"hydra_gallery\",\n",
    "      \"entity_type\": \"gallery\",\n",
    "      \"entity_role\": \"seller\",\n",
    "      \"event_id\": \"kylix_transaction_1985\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"extraction_class\": \"TRANSACTION\",\n",
    "    \"extraction_text\": \"sold\",\n",
    "    \"attributes\": {\n",
    "      \"transaction_type\": \"sale\",\n",
    "      \"date\": \"1985\",\n",
    "      \"seller_id\": \"hydra_gallery\",\n",
    "      \"buyer_id\": \"j_paul_getty_museum\",\n",
    "      \"artifact_id\": \"onesimos_kylix\",\n",
    "      \"amount\": \"$100,000\",\n",
    "      \"event_id\": \"kylix_transaction_1985\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"extraction_class\": \"ARTIFACT\",\n",
    "    \"extraction_text\": \"fragments of the Onesimos kylix\",\n",
    "    \"attributes\": {\n",
    "      \"canonical_id\": \"onesimos_kylix\",\n",
    "      \"object_type\": \"Greek pottery\",\n",
    "      \"condition\": \"fragmentary\",\n",
    "      \"event_id\": \"kylix_transaction_1985\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"extraction_class\": \"ORGANIZATION\",\n",
    "    \"extraction_text\": \"J. Paul Getty Museum\",\n",
    "    \"attributes\": {\n",
    "      \"canonical_id\": \"j_paul_getty_museum\",\n",
    "      \"entity_type\": \"museum\",\n",
    "      \"entity_role\": \"buyer\",\n",
    "      \"event_id\": \"kylix_transaction_1985\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Structured Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_structured(text, use_anthropic=True):\n",
    "    \"\"\"Extract entities using domain-specific structured schema\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"{EXTRACTION_SCHEMA}\n",
    "\n",
    "## Document to Extract:\n",
    "\n",
    "{text}\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Extract all entities from the document above following the schema.\n",
    "Return ONLY a JSON array of extractions, no other text.\n",
    "\n",
    "Format:\n",
    "[\n",
    "  {{\n",
    "    \"extraction_class\": \"<CLASS>\",\n",
    "    \"extraction_text\": \"<exact text from document>\",\n",
    "    \"attributes\": {{ ... }}\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if use_anthropic:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=16000,\n",
    "                temperature=0.1,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            result_text = response.content[0].text\n",
    "        else:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=0.1,\n",
    "                    max_output_tokens=8192,\n",
    "                )\n",
    "            )\n",
    "            result_text = response.text\n",
    "        \n",
    "        # Extract JSON from response (handle markdown code blocks)\n",
    "        result_text = result_text.strip()\n",
    "        if result_text.startswith('```'):\n",
    "            # Remove markdown code blocks\n",
    "            result_text = re.sub(r'^```json\\s*', '', result_text)\n",
    "            result_text = re.sub(r'^```\\s*', '', result_text)\n",
    "            result_text = re.sub(r'```\\s*$', '', result_text)\n",
    "        \n",
    "        extractions = json.loads(result_text)\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {len(extractions)} entities\")\n",
    "        \n",
    "        # Count by class\n",
    "        class_counts = defaultdict(int)\n",
    "        for ext in extractions:\n",
    "            class_counts[ext['extraction_class']] += 1\n",
    "        \n",
    "        print(\"\\nEntity breakdown:\")\n",
    "        for cls, count in sorted(class_counts.items()):\n",
    "            print(f\"  {cls}: {count}\")\n",
    "        \n",
    "        return extractions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(f\"Response text: {result_text[:500] if 'result_text' in locals() else 'N/A'}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kg_from_extractions(extractions):\n",
    "    \"\"\"Build knowledge graph from structured extractions\"\"\"\n",
    "    \n",
    "    nodes = {}\n",
    "    edges = []\n",
    "    \n",
    "    # First pass: Create nodes from entities with canonical IDs\n",
    "    entity_classes = ['PERSON', 'ORGANIZATION', 'ARTIFACT', 'LOCATION']\n",
    "    \n",
    "    for ext in extractions:\n",
    "        if ext['extraction_class'] in entity_classes:\n",
    "            attrs = ext.get('attributes', {})\n",
    "            canonical_id = attrs.get('canonical_id')\n",
    "            \n",
    "            if canonical_id:\n",
    "                # Create or update node\n",
    "                if canonical_id not in nodes:\n",
    "                    nodes[canonical_id] = {\n",
    "                        'id': canonical_id,\n",
    "                        'type': ext['extraction_class'],\n",
    "                        'label': attrs.get('full_name', ext['extraction_text']),\n",
    "                        'attributes': attrs,\n",
    "                        'mentions': [ext['extraction_text']]\n",
    "                    }\n",
    "                else:\n",
    "                    # Add new mention if not duplicate\n",
    "                    if ext['extraction_text'] not in nodes[canonical_id]['mentions']:\n",
    "                        nodes[canonical_id]['mentions'].append(ext['extraction_text'])\n",
    "    \n",
    "    # Second pass: Create relationships from events\n",
    "    # Group extractions by event_id\n",
    "    events = defaultdict(list)\n",
    "    for ext in extractions:\n",
    "        event_id = ext.get('attributes', {}).get('event_id')\n",
    "        if event_id:\n",
    "            events[event_id].append(ext)\n",
    "    \n",
    "    # Create edges based on events\n",
    "    for event_id, event_extractions in events.items():\n",
    "        # Find action/transaction in this event\n",
    "        action = None\n",
    "        for ext in event_extractions:\n",
    "            if ext['extraction_class'] in ['TRANSACTION', 'LEGAL_ACTION', 'CRIMINAL_ACTIVITY']:\n",
    "                action = ext\n",
    "                break\n",
    "        \n",
    "        if not action:\n",
    "            continue\n",
    "        \n",
    "        attrs = action.get('attributes', {})\n",
    "        action_type = attrs.get('transaction_type') or attrs.get('action_type') or attrs.get('activity_type')\n",
    "        \n",
    "        # Create edges based on action type\n",
    "        if action['extraction_class'] == 'TRANSACTION':\n",
    "            seller_id = attrs.get('seller_id')\n",
    "            buyer_id = attrs.get('buyer_id')\n",
    "            artifact_id = attrs.get('artifact_id')\n",
    "            \n",
    "            if seller_id and artifact_id:\n",
    "                edges.append({\n",
    "                    'source': seller_id,\n",
    "                    'target': artifact_id,\n",
    "                    'relation': f'sold ({attrs.get(\"date\", \"?\")})',\n",
    "                    'event_id': event_id,\n",
    "                    'attributes': attrs\n",
    "                })\n",
    "            \n",
    "            if buyer_id and artifact_id:\n",
    "                edges.append({\n",
    "                    'source': buyer_id,\n",
    "                    'target': artifact_id,\n",
    "                    'relation': f'acquired ({attrs.get(\"date\", \"?\")})',\n",
    "                    'event_id': event_id,\n",
    "                    'attributes': attrs\n",
    "                })\n",
    "            \n",
    "            if seller_id and buyer_id:\n",
    "                edges.append({\n",
    "                    'source': seller_id,\n",
    "                    'target': buyer_id,\n",
    "                    'relation': f'transaction ({attrs.get(\"date\", \"?\")})',\n",
    "                    'event_id': event_id,\n",
    "                    'attributes': attrs\n",
    "                })\n",
    "        \n",
    "        elif action['extraction_class'] == 'LEGAL_ACTION':\n",
    "            target_id = attrs.get('target_id')\n",
    "            authority = attrs.get('executing_authority')\n",
    "            \n",
    "            if target_id:\n",
    "                edges.append({\n",
    "                    'source': 'law_enforcement',\n",
    "                    'target': target_id,\n",
    "                    'relation': f'{action_type} ({attrs.get(\"date\", \"?\")})',\n",
    "                    'event_id': event_id,\n",
    "                    'attributes': attrs\n",
    "                })\n",
    "        \n",
    "        elif action['extraction_class'] == 'CRIMINAL_ACTIVITY':\n",
    "            artifact_id = attrs.get('artifact_id')\n",
    "            perpetrator_id = attrs.get('perpetrator_id')\n",
    "            \n",
    "            if perpetrator_id and artifact_id:\n",
    "                edges.append({\n",
    "                    'source': perpetrator_id,\n",
    "                    'target': artifact_id,\n",
    "                    'relation': action_type,\n",
    "                    'event_id': event_id,\n",
    "                    'attributes': attrs\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nüìä Knowledge Graph Stats:\")\n",
    "    print(f\"  Nodes: {len(nodes)}\")\n",
    "    print(f\"  Edges: {len(edges)}\")\n",
    "    print(f\"  Events: {len(events)}\")\n",
    "    \n",
    "    return {\n",
    "        'nodes': nodes,\n",
    "        'edges': edges,\n",
    "        'events': dict(events)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kg(kg, output_file=\"kg_visualization.png\", figsize=(16, 12)):\n",
    "    \"\"\"Create a visualization of the knowledge graph\"\"\"\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for node_id, node_data in kg['nodes'].items():\n",
    "        G.add_node(node_id, **node_data)\n",
    "    \n",
    "    # Add edges\n",
    "    for edge in kg['edges']:\n",
    "        G.add_edge(edge['source'], edge['target'], label=edge['relation'])\n",
    "    \n",
    "    # Create layout\n",
    "    plt.figure(figsize=figsize)\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Color nodes by type\n",
    "    color_map = {\n",
    "        'PERSON': '#FF6B6B',\n",
    "        'ORGANIZATION': '#4ECDC4',\n",
    "        'ARTIFACT': '#FFE66D',\n",
    "        'LOCATION': '#95E1D3'\n",
    "    }\n",
    "    \n",
    "    node_colors = [color_map.get(G.nodes[node].get('type', 'UNKNOWN'), '#CCCCCC') \n",
    "                   for node in G.nodes()]\n",
    "    \n",
    "    # Draw\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                           node_size=2000, alpha=0.9)\n",
    "    nx.draw_networkx_labels(G, pos, \n",
    "                            labels={n: G.nodes[n].get('label', n) for n in G.nodes()},\n",
    "                            font_size=8)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                           arrows=True, arrowsize=15, alpha=0.6)\n",
    "    \n",
    "    # Add edge labels\n",
    "    edge_labels = {(e['source'], e['target']): e['relation'] \n",
    "                   for e in kg['edges']}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved visualization to {output_file}\")\n",
    "    plt.show()\n",
    "\n",
    "def display_kg_summary(kg):\n",
    "    \"\"\"Display a text summary of the knowledge graph\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KNOWLEDGE GRAPH SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Group nodes by type\n",
    "    by_type = defaultdict(list)\n",
    "    for node_id, node in kg['nodes'].items():\n",
    "        by_type[node['type']].append((node_id, node))\n",
    "    \n",
    "    for entity_type, nodes_list in sorted(by_type.items()):\n",
    "        print(f\"\\n{entity_type} ({len(nodes_list)}):\")\n",
    "        for node_id, node in sorted(nodes_list)[:10]:  # Show first 10\n",
    "            label = node['label']\n",
    "            mentions = len(node.get('mentions', []))\n",
    "            print(f\"  ‚Ä¢ {label} (id: {node_id}, {mentions} mentions)\")\n",
    "    \n",
    "    print(f\"\\nRELATIONSHIPS ({len(kg['edges'])}):\")\n",
    "    for edge in kg['edges'][:15]:  # Show first 15\n",
    "        source_label = kg['nodes'].get(edge['source'], {}).get('label', edge['source'])\n",
    "        target_label = kg['nodes'].get(edge['target'], {}).get('label', edge['target'])\n",
    "        print(f\"  ‚Ä¢ {source_label} --[{edge['relation']}]--> {target_label}\")\n",
    "    \n",
    "    if len(kg['edges']) > 15:\n",
    "        print(f\"  ... and {len(kg['edges']) - 15} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_kg_json(kg, filename=\"kg_export.json\"):\n",
    "    \"\"\"Save knowledge graph as JSON\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(kg, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved knowledge graph to {filename}\")\n",
    "\n",
    "def export_to_neo4j_cypher(kg, filename=\"neo4j_import.cypher\"):\n",
    "    \"\"\"Export as Neo4j Cypher statements\"\"\"\n",
    "    \n",
    "    cypher = []\n",
    "    \n",
    "    # Create nodes\n",
    "    for node_id, node in kg['nodes'].items():\n",
    "        label = node['type']\n",
    "        props = {\n",
    "            'id': node_id,\n",
    "            'label': node['label'],\n",
    "            'mentions': node.get('mentions', [])\n",
    "        }\n",
    "        props.update(node.get('attributes', {}))\n",
    "        \n",
    "        props_str = ', '.join([f\"{k}: {json.dumps(v)}\" for k, v in props.items()])\n",
    "        cypher.append(f\"CREATE (:{label} {{{props_str}}})\")\n",
    "    \n",
    "    # Create relationships\n",
    "    for edge in kg['edges']:\n",
    "        rel_type = edge['relation'].replace(' ', '_').replace('(', '').replace(')', '').upper()\n",
    "        props_str = ', '.join([f\"{k}: {json.dumps(v)}\" \n",
    "                               for k, v in edge.get('attributes', {}).items()])\n",
    "        \n",
    "        cypher.append(\n",
    "            f\"MATCH (a {{id: {json.dumps(edge['source'])}}}), \"\n",
    "            f\"(b {{id: {json.dumps(edge['target'])}}}) \"\n",
    "            f\"CREATE (a)-[:{rel_type} {{{props_str}}}]->(b)\"\n",
    "        )\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\\n\".join(cypher))\n",
    "    \n",
    "    print(f\"‚úÖ Saved Neo4j Cypher to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document\n",
    "document_text = \"\"\"Giacomo Medici is an Italian antiquities dealer who was convicted in 2005 of receiving stolen goods, illegal export of goods, and conspiracy to traffic.\n",
    "\n",
    "Medici started dealing in antiquities in Rome during the 1960s. In July 1967, he was convicted in Italy of receiving looted artefacts, though in the same year he met and became an important supplier of antiquities to US dealer Robert Hecht. In 1968, Medici opened the gallery Antiquaria Romana in Rome and began to explore business opportunities in Switzerland.\n",
    "\n",
    "In 1978, he closed his Rome gallery, and entered into partnership with Geneva resident Christian Boursaud, who started consigning material supplied by Medici for sale at Sotheby's London. Together, they opened Hydra Gallery in Geneva in 1983.\n",
    "\n",
    "In October 1985, the Hydra Gallery sold fragments of the Onesimos kylix to the J. Paul Getty Museum for $100,000, providing a false provenance by way of the fictitious Zbinden collection. The Getty returned the kylix to Italy in 1999.\n",
    "\n",
    "It is widely believed that in December 1971 he bought the illegally-excavated Euphronios (Sarpedon) krater from tombaroli before transporting it to Switzerland and selling it to Hecht.\n",
    "\n",
    "On 13 September 1995, in concert with Swiss police, they raided Medici's storage space in the Geneva Freeport, which comprised five rooms with a combined area of about 200 sq metres.\n",
    "\n",
    "In January 1997, Medici was arrested in Rome. Medici was charged with receiving stolen goods, illegal export of goods, and conspiracy to traffic. On 12 May 2005, he was found guilty of all charges. He was sentenced to ten years in prison and received a ‚Ç¨10 million fine.\"\"\"\n",
    "\n",
    "# Extract entities\n",
    "print(\"Starting extraction...\\n\")\n",
    "extractions = extract_entities_structured(document_text, use_anthropic=USE_ANTHROPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build knowledge graph\n",
    "kg = build_kg_from_extractions(extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "display_kg_summary(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "visualize_kg(kg, \"antiquities_kg_enhanced.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs\n",
    "save_kg_json(kg, \"antiquities_kg_enhanced.json\")\n",
    "export_to_neo4j_cypher(kg, \"neo4j_import.cypher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all transactions\n",
    "print(\"\\nüì¶ TRANSACTIONS:\")\n",
    "for ext in extractions:\n",
    "    if ext['extraction_class'] == 'TRANSACTION':\n",
    "        attrs = ext['attributes']\n",
    "        print(f\"  ‚Ä¢ {attrs.get('seller_id', '?')} --[{ext['extraction_text']}]--> \"\n",
    "              f\"{attrs.get('buyer_id', '?')} [{attrs.get('date', '?')}]\")\n",
    "\n",
    "# Find all legal actions\n",
    "print(\"\\n‚öñÔ∏è LEGAL ACTIONS:\")\n",
    "for ext in extractions:\n",
    "    if ext['extraction_class'] == 'LEGAL_ACTION':\n",
    "        attrs = ext['attributes']\n",
    "        print(f\"  ‚Ä¢ {attrs.get('action_type', '?')}: {attrs.get('target_id', '?')} \"\n",
    "              f\"[{attrs.get('date', '?')}]\")\n",
    "\n",
    "# Find all artifacts\n",
    "print(\"\\nüè∫ ARTIFACTS:\")\n",
    "for node_id, node in kg['nodes'].items():\n",
    "    if node['type'] == 'ARTIFACT':\n",
    "        print(f\"  ‚Ä¢ {node['label']} (id: {node_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Process Your Own Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your document\n",
    "# Option 1: From file\n",
    "# with open('your_document.txt', 'r', encoding='utf-8') as f:\n",
    "#     your_document = f.read()\n",
    "\n",
    "# Option 2: Paste directly\n",
    "your_document = \"\"\"Paste your antiquities trafficking document here...\"\"\"\n",
    "\n",
    "# Extract\n",
    "your_extractions = extract_entities_structured(your_document, use_anthropic=USE_ANTHROPIC)\n",
    "\n",
    "# Build graph\n",
    "your_kg = build_kg_from_extractions(your_extractions)\n",
    "\n",
    "# Display and save\n",
    "display_kg_summary(your_kg)\n",
    "visualize_kg(your_kg, \"my_kg.png\")\n",
    "save_kg_json(your_kg, \"my_kg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Batch Processing Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_documents(document_list, use_anthropic=True):\n",
    "    \"\"\"Process multiple documents and merge into single knowledge graph\"\"\"\n",
    "    \n",
    "    all_extractions = []\n",
    "    \n",
    "    for i, doc in enumerate(document_list, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing document {i}/{len(document_list)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        extractions = extract_entities_structured(doc, use_anthropic=use_anthropic)\n",
    "        all_extractions.extend(extractions)\n",
    "    \n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"MERGING {len(all_extractions)} TOTAL EXTRACTIONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build unified knowledge graph\n",
    "    merged_kg = build_kg_from_extractions(all_extractions)\n",
    "    \n",
    "    return merged_kg, all_extractions\n",
    "\n",
    "# Example usage:\n",
    "# documents = [\n",
    "#     \"Document 1 text...\",\n",
    "#     \"Document 2 text...\",\n",
    "#     \"Document 3 text...\"\n",
    "# ]\n",
    "# merged_kg, all_exts = process_multiple_documents(documents)\n",
    "# display_kg_summary(merged_kg)\n",
    "# visualize_kg(merged_kg, \"merged_kg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "This enhanced notebook provides:\n",
    "\n",
    "### Key Features:\n",
    "1. **Domain-Specific Schema**: 8 entity classes tailored for antiquities trafficking\n",
    "2. **Coreference Resolution**: Canonical IDs link mentions of same entity\n",
    "3. **Event Linking**: Shared event_id connects related extractions\n",
    "4. **Order Preservation**: Entities extracted in order of appearance\n",
    "5. **Rich Attributes**: Contextual metadata for each extraction\n",
    "\n",
    "### Entity Classes:\n",
    "- PERSON (dealers, collectors, officials)\n",
    "- ORGANIZATION (museums, galleries, law enforcement)\n",
    "- ARTIFACT (trafficked objects)\n",
    "- CRIMINAL_ACTIVITY (illicit actions)\n",
    "- TRANSACTION (sales, acquisitions)\n",
    "- LEGAL_ACTION (raids, convictions)\n",
    "- LOCATION (trafficking network locations)\n",
    "- PROVENANCE_CLAIM (false histories)\n",
    "\n",
    "### Next Steps:\n",
    "- Fine-tune extraction prompts for your specific corpus\n",
    "- Add temporal analysis and timeline visualization\n",
    "- Implement network analysis (centrality, communities)\n",
    "- Connect to Neo4j for advanced graph queries\n",
    "- Build web interface for interactive exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
